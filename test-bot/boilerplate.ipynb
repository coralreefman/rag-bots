{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException\n",
    "from fastapi.responses import FileResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from typing import Sequence\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\"Static configuration and instances of models and embeddings\"\"\"\n",
    "    \n",
    "    # Initialize model instances once\n",
    "    _MODELS = {\n",
    "        \"openai-gpt-4o\": {\n",
    "            \"name\": \"gpt-4o\",\n",
    "            \"provider\": \"OpenAI API\",\n",
    "            \"description\": \"Most capable OpenAI model\",\n",
    "            \"instance\": ChatOpenAI(model=\"gpt-4o\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "    _EMBEDDINGS = {\n",
    "        \"openai-default\": {\n",
    "            \"name\": \"text-embedding-3-large\",\n",
    "            \"provider\": \"OpenAI\",\n",
    "            \"description\": \"Default OpenAI embedding model\",\n",
    "            \"instance\": OpenAIEmbeddings()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_llm(cls, model_id):\n",
    "        \"\"\"Get existing LLM instance\"\"\"\n",
    "        if model_id not in cls._MODELS:\n",
    "            raise ValueError(f\"Unknown LLM: {model_id}\")\n",
    "        return cls._MODELS[model_id][\"instance\"]\n",
    "    \n",
    "    @classmethod\n",
    "    def get_embeddings(cls, embedding_id):\n",
    "        \"\"\"Get existing embeddings instance\"\"\"\n",
    "        if embedding_id not in cls._EMBEDDINGS:\n",
    "            raise ValueError(f\"Unknown embeddings: {embedding_id}\")\n",
    "        return cls._EMBEDDINGS[embedding_id][\"instance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_ID = \"openai-gpt-4o\"\n",
    "EMBEDDINGS_ID = \"openai-default\"\n",
    "LLM = ModelConfig.get_llm(\"openai-gpt-4o\") \n",
    "EMBEDDINGS = ModelConfig.get_embeddings(\"openai-default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should probably add Grading prompt here as well\n",
    "\n",
    "def setup_prompts():\n",
    "    \"\"\"Setup chat prompts\"\"\"\n",
    "    system_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Your name is Talin. You're part of the HOME0001 team. You are an expert customer service rep for the housing collective HOME0001. Your goal is to help customers who are looking to buy a home at one of our locations. Use the following pieces of retrieved context to answer any question. If you don't find an answer just admit that you don't know. Only answer questions related to HOME0001. Talk in a pragmatic tone. don't get cute, keep it professional but avoid marketing or corporate speak at all costs. Talk like you're texting. Use three sentences maximum and keep the answer concise. Do not admit that you're a bot and do not give it away. Always act human. Talk about HOME0001 as 'us' and 'we'. \\n\\n{context}\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "    \n",
    "    context_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    return system_prompt, context_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT, CONTEXT_PROMPT = setup_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumb JSON Document Loader - To be replaced\n",
    "def load_documents(file_path):\n",
    "        \"\"\"Load QA documents\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            return [Document(page_content=item[\"answer\"], \n",
    "                        metadata={\"question\": item[\"question\"]}) \n",
    "                for item in json.load(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_path = \"../data/home0001qa.json\"\n",
    "DOCUMENTS = load_documents(documents_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_vector_db(documents):\n",
    "\n",
    "    if os.path.isfile(f\"./FAISS/{EMBEDDINGS_ID}.faiss\"):\n",
    "            \n",
    "            vectorstore = FAISS.load_local(\n",
    "                folder_path=\"./FAISS\",\n",
    "                embeddings=EMBEDDINGS,\n",
    "                index_name=EMBEDDINGS_ID,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "    else: \n",
    "        vectorstore = FAISS.from_documents(\n",
    "            documents,\n",
    "            EMBEDDINGS\n",
    "        )\n",
    "        vectorstore.save_local(\"./FAISS\", EMBEDDINGS_ID)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORSTORE = setup_vector_db(DOCUMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement grader into rag chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm_model = \"llama3.2:3b-instruct-fp16\"\n",
    "\n",
    "llm_json_mode = ChatOllama(model=llm_model, temperature=0, format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc grader instructions\n",
    "doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "\n",
    "# Grader prompt\n",
    "doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(documents, question):\n",
    "  \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "\n",
    "    for d in documents:\n",
    "        doc_grader_prompt_formatted = doc_grader_prompt.format(\n",
    "            document=d.page_content, question=question\n",
    "        )\n",
    "        result = llm_json_mode.invoke(\n",
    "            [SystemMessage(content=doc_grader_instructions)]\n",
    "            + [HumanMessage(content=doc_grader_prompt_formatted)]\n",
    "        )\n",
    "        grade = json.loads(result.content)[\"binary_score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            continue\n",
    "\n",
    "    return {\"documents\": filtered_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n"
     ]
    }
   ],
   "source": [
    "test_retriever = VECTORSTORE.as_retriever()\n",
    "question = \"Do i own my 0001 home outright?\"\n",
    "retrieved_docs = test_retriever.invoke(question)\n",
    "filtered_docs = grade_documents(retrieved_docs, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shall we add prompt evaluation here?\n",
    "\n",
    "def initialize_rag_chain():\n",
    "        \n",
    "    retriever = VECTORSTORE.as_retriever()\n",
    "    \n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        LLM, retriever, CONTEXT_PROMPT\n",
    "    )\n",
    "    system_chain = create_stuff_documents_chain(LLM, SYSTEM_PROMPT)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, system_chain)\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_CHAIN = initialize_rag_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statefully manage chat history ###\n",
    "# -> Check again how this works exactly\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: State):\n",
    "    response = RAG_CHAIN.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_app():\n",
    "\n",
    "    # TO DO: add document grade to check for retrieved doc relevance AND \n",
    "    # add case for when no doc is relevant\n",
    "    \n",
    "    workflow = StateGraph(state_schema=State)\n",
    "    workflow.add_edge(START, \"model\")\n",
    "    workflow.add_node(\"model\", call_model)\n",
    "\n",
    "    memory = MemorySaver()\n",
    "\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_APP = initialize_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_response(message: str, config: dict) -> str:\n",
    "    \"\"\"Get response for a message\"\"\"\n",
    "    try:\n",
    "        response = LANGCHAIN_APP.invoke(\n",
    "            {\"input\": message},\n",
    "            config=config\n",
    "        )\n",
    "        return response.get(\"answer\", \"Warning: Response object missing 'answer' field. Please check the invocation process.\")\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Frank, how can I help you today?\n"
     ]
    }
   ],
   "source": [
    "config_01 =  {\"configurable\": {\"thread_id\": \"test-id-05\"}}\n",
    "response_01 = await get_response(\"hey what's up, i'm frank\", config_01)\n",
    "print(response_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey Lutz, how can I help you today?\n"
     ]
    }
   ],
   "source": [
    "config_02 =  {\"configurable\": {\"thread_id\": \"test-id-06\"}}\n",
    "response_02 = await get_response(\"hey what's up, i'm lutz\", config_02)\n",
    "print(response_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You mentioned your name is Frank. How can I assist you with your home search?\n"
     ]
    }
   ],
   "source": [
    "config_02 =  {\"configurable\": {\"thread_id\": \"test-id-05\"}}\n",
    "response_02 = await get_response(\"what's my name?\", config_02)\n",
    "print(response_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You mentioned your name is Frank. How can I assist you with your home search?\n"
     ]
    }
   ],
   "source": [
    "config_01 =  {\"configurable\": {\"thread_id\": \"test-id-06\"}}\n",
    "response_01 = await get_response(\"what's my name?\", config_01)\n",
    "print(response_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "LANGCHAIN_APP probably has to be reinitialized per thread, otherwise conversations seem to get mixed up.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
